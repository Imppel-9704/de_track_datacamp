{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeIJLahzzAPzTcPxJm9rsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imppel-9704/de_track_datacamp/blob/main/l12_Cleaning_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Data in Python\n",
        "\n",
        "## Common data problems\n",
        "### Why do we need to clean data?\n",
        "Dirty data can appear because of\n",
        "1. duplicate values\n",
        "2. mis-spellings\n",
        "3. data type parsing errors\n",
        "4. legacy systems\n",
        "\n",
        "Without making sure that data is properly cleaned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated. Garbage in Garbage out.\n",
        "\n",
        "### Data type constraints\n",
        "Data type\n",
        "- Text data: First name, Last name, address\n",
        "- Integers: # Subscribers, # products sold\n",
        "- Decimals: Temperature, Exchange rates\n",
        "- Binary: Is married?, New customer?, yes/no\n",
        "- Dates: Order dates, ship dates\n",
        "- Categories: Marriage status, Gender\n",
        "\n",
        "Python data type\n",
        "- str\n",
        "- int\n",
        "- float\n",
        "- bool\n",
        "- datetime\n",
        "- category\n",
        "\n",
        "Get dataframe's data types.\n",
        "```\n",
        "df.dtypes\n",
        "```\n",
        "\n",
        "Get dataframe information\n",
        "```\n",
        "df.info()\n",
        "```\n",
        "\n",
        "str to int\n",
        "```\n",
        "# remove $ from revenue column\n",
        "df['revenue'] = df['revenue'].str.strip('$')\n",
        "# convert str to int\n",
        "df['revenue'] = df['revenue'].astype('int')\n",
        "\n",
        "# Make sure that the revenue col is now an int by using the assert statement\n",
        "assert df['revenue'].dtype == 'int'\n",
        "\n",
        "# sum\n",
        "df['revenue'].sum()\n",
        "```\n",
        "\n",
        "Numeric or categorical?\n",
        "```\n",
        "# convert to categorical\n",
        "df['marriage_status'] = df['marriage_status'].astype('category')\n",
        "# return statistical summary\n",
        "df.describe()\n",
        "```"
      ],
      "metadata": {
        "id": "ZW27m5DlrwJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywangUvNrvUG"
      },
      "outputs": [],
      "source": [
        "assert 1+1 == 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 1+1 == 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "n-1dTwPM8e-S",
        "outputId": "2b7440d8-655e-433a-a721-eaf5d1309720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-968c1c7d70ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*insert, convert to category into new column*\n",
        "```\n",
        "# Print the information of ride_sharing\n",
        "print(ride_sharing.info())\n",
        "\n",
        "# Print summary statistics of user_type column\n",
        "print(ride_sharing['user_type'].describe())\n",
        "\n",
        "# Convert user_type from integer to category\n",
        "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
        "\n",
        "# Write an assert statement confirming the change\n",
        "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
        "\n",
        "# Print new summary statistics\n",
        "print(ride_sharing['user_type_cat'].describe())\n",
        "```\n",
        "\n",
        "*remove word \"minutes\" from column and convert from str to int*\n",
        "```\n",
        "# Strip duration of minutes\n",
        "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip(\"minutes\")\n",
        "\n",
        "# Convert duration to integer\n",
        "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
        "\n",
        "# Write an assert statement making sure of conversion\n",
        "assert ride_sharing['duration_time'].dtype == 'int'\n",
        "\n",
        "# Print formed columns and calculate average ride duration\n",
        "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
        "print(ride_sharing['duration_time'].mean())\n",
        "```"
      ],
      "metadata": {
        "id": "iY62qIO19_xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data range constraints\n",
        "\n",
        "Can future sign-ups exist? It actually shouldn't return any output.\n",
        "```\n",
        "# import data time\n",
        "import datetime as dt\n",
        "today_date = dt.date.today()\n",
        "user_signups[user_signups['subscription_date'] > today_date]\n",
        "```\n",
        "\n",
        "### How to deal with out of range data?\n",
        "- Dropping data\n",
        "- Setting custom minimums and maximums\n",
        "- Teat as missing and imput\n",
        "- Setting custom value depending on business assumptions.\n",
        "\n",
        "Movie example\n",
        "```\n",
        "import pandas as pd\n",
        "# Output movies with rating should not be > 5\n",
        "movies[movies['avg_rating'] > 5]\n",
        "\n",
        "# There are 3 ways to get rid of rating > 5\n",
        "# 1. drop values using filtering\n",
        "movies = movies[movies['avg_rating'] <= 5]\n",
        "# 2. drop values using .drop()\n",
        "movies.drop(movies[movies['avg_rating'] > 5].index, inplace=True)\n",
        "# 3. convert > 5 to 5\n",
        "movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5\n",
        "```\n",
        "\n",
        "### How to convert to datetime\n",
        "```\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "# Convert to date\n",
        "df['subscription_date'] = pd.to_datetime(df['subscription_date']).dt.date\n",
        "```\n",
        "\n",
        "Clean date\n",
        "```\n",
        "import datetime as dt\n",
        "today_date = dt.date.today()\n",
        "# drop values using filtering\n",
        "df = df[df['subscription_date'] < today_date]\n",
        "# using .drop()\n",
        "df.drop(df[df['subscription_date'] > today_date].index, inplace=True)\n",
        "```\n",
        "\n",
        "Hardcodes date with upper limit\n",
        "```\n",
        "## Drop values using filtering, replace with today_date\n",
        "df.loc[df['subscription_date'] > today_date, 'subscription_date'] = today_date\n",
        "```\n",
        "\n",
        "More example:\n",
        "```\n",
        "# Convert tire_sizes to integer\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
        "\n",
        "# Set all values above 27 to 27\n",
        "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
        "\n",
        "# Reconvert tire_sizes back to categorical\n",
        "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
        "\n",
        "# Print tire size description\n",
        "print(ride_sharing['tire_sizes'].describe())\n",
        "```\n",
        "\n",
        "Convert to datetime example:\n",
        "```\n",
        "# Convert ride_date to date\n",
        "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
        "\n",
        "# Save today's date\n",
        "today = dt.date.today()\n",
        "\n",
        "# Set all in the future to today's date\n",
        "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
        "\n",
        "# Print maximum of ride_dt column\n",
        "print(ride_sharing['ride_dt'].max())\n",
        "```"
      ],
      "metadata": {
        "id": "fz_kbnrf_NTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uniqueness constraints\n",
        "\n",
        "What is duplicate values?\n",
        "- All columns have the same values.\n",
        "- Most columns have the same vaues.\n",
        "\n",
        "Why do they happen?\n",
        "1. Data entry and Human Error\n",
        "2. Bugs and design errors\n",
        "3. Join or merge errors\n",
        "\n",
        "How to find duplicate values?\n",
        "```\n",
        "# get duplicate across all columns\n",
        "duplicates = df.duplicated()\n",
        "print(duplicates)\n",
        "## output will be True or False\n",
        "```\n",
        "\n",
        "See exactly\n",
        "```\n",
        "# get duplicated rows\n",
        "duplicates = df.duplicated()\n",
        "df[duplicates]\n",
        "\n",
        "## output will show all duplicated values except for the first occurrences.\n",
        "```\n",
        "\n",
        "### How to find duplicate rows?\n",
        "The .duplicated() method\n",
        "- subset: List of colum names to check for duplication.\n",
        "- keep: Whether to keep first ('first'), last ('last') or all (False) duplicate values.\n",
        "\n",
        "```\n",
        "# column names to check for duplication\n",
        "col = ['first_name', 'last_name', 'address']\n",
        "duplicates = df.duplicated(subet=col, keep=False)\n",
        "# output duplicate values\n",
        "df[duplicates].sort_values(by='first_name')\n",
        "```\n",
        "\n",
        "### How to treat duplicate values?\n",
        "The .drop_duplicates() method\n",
        "- subset: List of colum names to check for duplication.\n",
        "- keep: Whether to keep first ('first'), last ('last') or all (False) duplicate values.\n",
        "- inplace: Drop duplicated rows directly indside DataFrame without creating new object (True).\n",
        "\n",
        "```\n",
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)\n",
        "```\n",
        "\n",
        "There are some duplicated columns as output (columns first_name, last_name are the same. only just weight col is differrent.) so try using .gropby() and agg()\n",
        "\n",
        "### Group by and Aggregation\n",
        "The .groupby() and .agg() methods \\\n",
        "Group by a set of common columns and return statistical values for specific columns when the aggregation is being performed.\n",
        "```\n",
        "# Group by column name and produce statistical summaries\n",
        "col = ['first_name', 'last_name', 'address']\n",
        "# create dict\n",
        "summaries = {'height': 'max', 'weight': 'mean'}\n",
        "# we can have numbered indices in the final output by using .reset_index()\n",
        "height_weight = height_weight.groupby(by=col).agg(summaries).reset_index()\n",
        "\n",
        "# Make sure aggregation is don\n",
        "duplicates = height_weight.duplicates(subset=col, keep=False)\n",
        "height_weight[duplicates].sort_values(by='first_name')\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "find duplicate\n",
        "```\n",
        "# Find duplicates\n",
        "duplicates = ride_sharing.duplicated(subset='ride_id', keep=False)\n",
        "\n",
        "# Sort your duplicated rides\n",
        "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
        "\n",
        "# Print relevant columns of duplicated_rides\n",
        "print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
        "```\n",
        "\n",
        "drop duplicate\n",
        "```\n",
        "# Drop complete duplicates from ride_sharing\n",
        "ride_dup = ride_sharing.drop_duplicates(inplace=True)\n",
        "\n",
        "# Create statistics dictionary for aggregation function\n",
        "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
        "\n",
        "# Group by ride_id and compute new statistics\n",
        "ride_unique = ride_dup.groupby(by=statistics).agg('ride_id').reset_index()\n",
        "\n",
        "# Find duplicated values again\n",
        "duplicates = ride_unique.duplicates(subset = 'ride_id', keep = False)\n",
        "duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "# Assert duplicates are processed\n",
        "assert duplicated_rides.shape[0] == 0\n",
        "```"
      ],
      "metadata": {
        "id": "nHWLu9UsEqpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text and categorical data problems\n",
        "\n",
        "### Categories and membership constraints\n",
        "Predefined finite set of categories.\n",
        "- Type of data: Household Income\n",
        "- Example: 0-20k, 20-40k, ...\n",
        "- Numeric representation: 0, 1, ...\n",
        "\n",
        "### Why could we have these problems?\n",
        "This could be due to\n",
        "- data entry issues\n",
        "- data parsing errors\n",
        "\n",
        "### How do we treat these problems?\n",
        "- Dropping data\n",
        "- Remapping Categories\n",
        "- Inferring Categories\n",
        "\n",
        "Dropping data example\n",
        "```\n",
        "# Import data\n",
        "study_data = pd.read_csv('study.csv')\n",
        "study_data.dtypes\n",
        "\n",
        "# columns blood_type's type is category but have inconsistent value.\n",
        "```\n",
        "\n",
        "Finding inconsistent categories\n",
        "```\n",
        "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
        "print(inconsisten_categories)\n",
        "\n",
        "## output will return all the categories in blood_type that are not in categories (inconsistent value)\n",
        "```\n",
        "\n",
        "dropping inconsistent categories\n",
        "```\n",
        "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories) # return True or False for each rows\n",
        "inconsistent_data = study_data[inconsistent_rows]\n",
        "# Drop inconsistent categories and get consistent data only\n",
        "consistent_date = study_data[~inconsistent_rows]\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Print categories from DataFrame\n",
        "print(categories)\n",
        "\n",
        "# Print unique values of survey columns in airlines\n",
        "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', airlines['satisfaction'].unique(),\"\\n\")\n",
        "```\n",
        "\n",
        "find inconsistency\n",
        "```\n",
        "# Find the cleanliness category in airlines not in categories\n",
        "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
        "\n",
        "# Find rows with that category\n",
        "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Print rows with inconsistent category\n",
        "print(airlines[cat_clean_rows])\n",
        "\n",
        "## output will be only rows that contains inconsistency value\n",
        "```\n",
        "\n",
        "Print rows with consistent categories only\n",
        "```\n",
        "# Print rows with consistent categories only\n",
        "print(airlines[~cat_clean_rows])\n",
        "```\n",
        "\n",
        "## Categorical variables\n",
        "### What type of errors could we have\n",
        "1. Value inconsistency\n",
        "  - Inconsistency fields: 'married', 'Maried', 'UNMARRIED', 'not married', ...\n",
        "  - _Trailing white spaces: 'married ', ' married ' ..\n",
        "2. Collapsing too many categories to few\n",
        "  - Creating new groups: 0-20k, 20-40k categories ... from continuos household income data\n",
        "  - Mapping groups to new ones: Mapping household income categories to 2 'rich', 'poor'\n",
        "3. Making sure data is of type category\n",
        "\n",
        "### Value consistency\n",
        "Capitalization: 'married', 'Married', 'UNMARRIED', 'unmarried' ..\n",
        "```\n",
        "# Get marriage status column\n",
        "marriage_status = demographic['marriage_status']\n",
        "marriage_status.value_count() # .value_count() works on series only\n",
        "\n",
        "# get value counts on DataFrame\n",
        "marriage_status.groupby('marriage_status').count()\n",
        "```\n",
        "\n",
        "To deal with capitalization use this \\\n",
        "Capitalie\n",
        "```\n",
        "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()\n",
        "marriage_status['marriage_status'].value_count()\n",
        "```\n",
        "Lower case\n",
        "```\n",
        "marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()\n",
        "marriage_status['marriage_status'].value_count()\n",
        "```\n",
        "\n",
        "In case DataFrame contains with leading spaces. \\\n",
        "Strip all space\n",
        "```\n",
        "demographic = demographic['marriage_status'].str.strip()\n",
        "demographic['marriage_status'].value_counts()\n",
        "```\n",
        "\n",
        "### Collapsing data into categories\n",
        "Creating categories out of data: income_group column from income column. \\\n",
        "\\\n",
        "pd.qcut()\n",
        "```\n",
        "# Using qcut()\n",
        "import pandas as pd\n",
        "group_names = ['0-200k', '200k-500k', '500k+']\n",
        "demographics['income_group'] = pd.qcut(demographics['household_income'], q=3, labels=group_names)\n",
        "\n",
        "# Print income_group column\n",
        "demographics[['income_group', 'household_income']]\n",
        "```\n",
        "\\\n",
        "pd.cut()\n",
        "```\n",
        "# Using cut() - create category range and names\n",
        "ranges = [0, 200000, 500000, np.inf] # np.inf will represent the final one being infinity.\n",
        "# Create income group column\n",
        "demographics['income_group'] = pd.cut(demographics['household_income'], bin=ranges, labels=group_names) # bin will take in a list of cutoff points for each category\n",
        "demographics[['income_group', 'household_income']]\n",
        "```\n",
        "\n",
        "Reduce the amount of data we have in our data. \\\n",
        "Mapping categories to fewer one\n",
        "- OS columns is: 'Microsoft', 'MacOS', 'IOS', 'Android', 'Linux'\n",
        "- OS column should become: 'DesktopOS', 'MobileOS'\n",
        "\n",
        "```\n",
        "# Create mapping dict and replace\n",
        "mapping = {'Microsoft': 'DesktopOS', 'MacOS': 'DesktopOS', 'IOS': 'MobileOS', 'Android': 'MobileOS', 'Linux': 'DesktopOS'}\n",
        "devices['OS'] = devices['OS'].replace(mapping)\n",
        "devices['OS'].unique()\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "1\n",
        "```\n",
        "# Print unique values of both columns\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "\n",
        "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
        "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
        "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
        "\n",
        "# Remove white spaces from `dest_size`\n",
        "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
        "\n",
        "# Verify changes have been effected\n",
        "print(airlines['dest_region'].unique())\n",
        "print(airlines['dest_size'].unique())\n",
        "```\n",
        "\n",
        "2\n",
        "```\n",
        "# Create ranges for categories\n",
        "label_ranges = [0, 60, 180, np.inf]\n",
        "label_names = ['short', 'medium', 'long']\n",
        "\n",
        "# Create wait_type column\n",
        "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges,\n",
        "                                labels = label_names)\n",
        "\n",
        "# Create mappings and replace\n",
        "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday',\n",
        "            'Thursday': 'weekday', 'Friday': 'weekday',\n",
        "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
        "\n",
        "airlines['day_week'] = airlines['day'].replace(mappings)\n",
        "```\n",
        "\n",
        "## Cleaning Text Data\n",
        "Text data is most common type \\\n",
        "problems\n",
        "1. Data inconsistency: +96171679912 or 0096171679912 or ..?\n",
        "2. Fixed length violations: Passwords needs to be at least 8 characters\n",
        "3. Typos: +961.71.679912\n",
        "\n",
        "### Example\n",
        "Fixing the phone number column\n",
        "```\n",
        "# replace \"+\" with \"00\"\n",
        "phones['phone_number'] = phones['phone_number'].str.replace('+', '00')\n",
        "phones\n",
        "```\n",
        "\\\n",
        "```\n",
        "# replace \"-\" with nothing\n",
        "phones['phone_number'] = phones['phone_number'].str.replace('-', '')\n",
        "phones\n",
        "```\n",
        "\\\n",
        "```\n",
        "# replace phone number with lower then 10 digits to NaN\n",
        "digit = phones['phone_number'].str.len()\n",
        "phones.loc[digit < 10, 'phone_number'] =np.nan\n",
        "phones\n",
        "```\n",
        "\n",
        "We can you assert statements to test wether the phone number column has specific lenght, and wether it contains the symbols we removed.\n",
        "```\n",
        "# Find length of each row in Phone number column\n",
        "sanity_check = phones['phone_number'].str.len()\n",
        "\n",
        "# Assert min phone number length is 10\n",
        "assert sanity_check.min() >= 10\n",
        "\n",
        "# Assert all numbers do not have \"+\" or \"-\"\n",
        "assert phones['phone_number'].str.contains(\"+|-\").any() == False\n",
        "```\n",
        "*Remember assert returns nothing if the condition passes*\n",
        "\n",
        "### What's about more complex examples?\n",
        "e.g. +(01706)-25891, +0500-571437, +0800-1111 \\\n",
        "\n",
        "### Use Regular expression!\n",
        "```\n",
        "# Replace letters with nothing\n",
        "phones['phone_number'] = phones['phone_number'].str.replace(r'\\D+', '')\n",
        "phones.head()\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "1\n",
        "```\n",
        "# Replace \"Dr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Dr.\",\"\")\n",
        "\n",
        "# Replace \"Mr.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Mr.\",\"\")\n",
        "\n",
        "# Replace \"Miss\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Miss\",\"\")\n",
        "\n",
        "# Replace \"Ms.\" with empty string \"\"\n",
        "airlines['full_name'] = airlines['full_name'].str.replace(\"Ms.\",\"\")\n",
        "\n",
        "# Assert that full_name has no honorifics\n",
        "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
        "```\n",
        "2\n",
        "```\n",
        "# Store length of each row in survey_response column\n",
        "resp_length = airlines['survey_response'].str.len()\n",
        "\n",
        "# Find rows in airlines where resp_length > 40\n",
        "airlines_survey = airlines[resp_length > 40]\n",
        "\n",
        "# Assert minimum survey_response length is > 40\n",
        "assert airlines_survey['survey_response'].str.len().min() > 40\n",
        "\n",
        "# Print new survey_response column\n",
        "print(airlines_survey['survey_response'])\n",
        "```"
      ],
      "metadata": {
        "id": "3_xXQyKGL8Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced data problems\n",
        "## Uniformity, Cross field validation and Dealing with missing data\n",
        "\n",
        "### Uniformity\n",
        "- Temp: 32C is also 89.6F\n",
        "- Weight: 70Kg is also 11st.\n",
        "- Date: 26-11-2019 is also 26, November, 2019\n",
        "\n",
        "We can try pyplot to plot scatter to identify uniformity. \\\n",
        "\n",
        "### Treating with Temp data\n",
        "Convert f to c\n",
        "```\n",
        "temp_fah = temperatures.loc[temperatures['Temperature'] > 40, 'Temperature']\n",
        "temp_cels = (temp_fah - 32) * (5/9)\n",
        "temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels\n",
        "\n",
        "# Assert to check conversion is correct\n",
        "assert temperatures['Temperature'].max() < 40\n",
        "```\n",
        "\n",
        "### Treating with date data\n",
        "If DataFrame contains variaty date format like 27/27/19, 03-29-19, March 3rd, 2019.\n",
        "\n",
        "```\n",
        "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'],\n",
        "                        # Attempt to infer format of each date\n",
        "                        infer_datetime_format=True,\n",
        "                        # Return NA for rows where conversion failed\n",
        "                        errors='coerce')\n",
        "```\n",
        "\n",
        "We can also convert datetime format using the dt.strftime\n",
        "```\n",
        "birthdays['Birthday'] = birthdays['Birthday'].dt.strftime(\"%d-%m-%Y\")\n",
        "birthdays.head()\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Print the header of account_opend\n",
        "print(banking['account_opened'].head())\n",
        "\n",
        "# Convert account_opened to datetime\n",
        "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
        "                                           # Infer datetime format\n",
        "                                           infer_datetime_format = True,\n",
        "                                           # Return missing value for error\n",
        "                                           errors = 'coerce')\n",
        "\n",
        "# Get year of account opened. Only pull year from col 'account_opened'\n",
        "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
        "\n",
        "# Print acct_year\n",
        "print(banking['acct_year'])\n",
        "```\n",
        "\n",
        "### Cross field validation\n",
        "The use of multiple field in a dataset to sanity check data integrity.\n",
        "```\n",
        "# use pandas to use subset to sum cols\n",
        "sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis=1)\n",
        "passenger_equ = sum_classes == flights['total_passenger']\n",
        "# find and filter out rows with inconsistent passenger total\n",
        "inconsistent_pass = flights[~passenger_equ]\n",
        "consistent_pass = fligts[passenger_equ]\n",
        "```\n",
        "\n",
        "Another example containg user_id, birthdays and age values.\n",
        "```\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "# Convert to datetime and get today's date\n",
        "users['Birthday'] = pd.to_datetime(users['Birthday'])\n",
        "today = dt.date.today()\n",
        "\n",
        "# For each row in the Birthday column, calculate year difference\n",
        "age_manual = today.year - users['Birthday'].dt.year\n",
        "\n",
        "# Find instances where ages match\n",
        "age_equ = age_manual == users['Age']\n",
        "\n",
        "# Find and filter out rows with inconsistent age\n",
        "inconsistent_age = users[~age_equ]\n",
        "consistent_age = users[age_equ]\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Store today's date and find ages\n",
        "today = dt.date.today()\n",
        "ages_manual = today.year - banking['birth_date'].dt.year\n",
        "\n",
        "# Find rows where age column == ages_manual\n",
        "age_equ = banking['age'] == ages_manual\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "consistent_ages = banking[age_equ]\n",
        "inconsistent_ages = banking[~age_equ]\n",
        "\n",
        "# Store consistent and inconsistent data\n",
        "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])\n",
        "```\n",
        "\n",
        "### Completeness\n",
        "### What is missing data?\n",
        "Occurs when no data value is stored for a variable in an observation Can be represented as NA, nan, 0, .\n",
        "\n",
        "```\n",
        "# To find missing value\n",
        "df.isna()\n",
        "\n",
        "# Get summary of missingness\n",
        "df.isna().sum()\n",
        "```\n",
        "\n",
        "Or can use Missingno to identify missing value\n",
        "```\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize missingno\n",
        "msno.matrix(df)\n",
        "plt.show()\n",
        "\n",
        "## the return will shows missing values are distributed across a column.\n",
        "```\n",
        "Isolate missing and complete values aside\n",
        "```\n",
        "# Isolate missing and complete values aside\n",
        "missing = df[df['col1'].isna() ]\n",
        "complete = df[~df['col1'].isna()]\n",
        "\n",
        "# Then use .describe() method\n",
        "missing.describe()\n",
        "complete.describe()\n",
        "```\n",
        "\n",
        "Confirming by using MSNO\n",
        "```\n",
        "sorted_data = df.sort_values(by='col2')\n",
        "msno.matrix(sorted_data)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### How to deal with missing data?\n",
        "Simple approaches:\n",
        "1. Drop missing data\n",
        "2. Impute with statistical measures (mean, median, mode.)\n",
        "More complex approaches:\n",
        "1. Imputing using an algorithmic approach\n",
        "2. Impute with machine learning models.\n",
        "\n",
        "\\\n",
        "\n",
        "Dropping missing value\n",
        "```\n",
        "df_dropped = df.dropna(subset=['col1'])\n",
        "df_dropped.head()\n",
        "```\n",
        "Replacing missing values with statistical measures\n",
        "```\n",
        "col1_mean = df['col1'].mean()\n",
        "df_imputed = df.fillna({'col1': col1_mean})\n",
        "df_imputed.head()\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "1\n",
        "```\n",
        "# Print number of missing values in banking\n",
        "print(banking.isna().sum())\n",
        "\n",
        "# Visualize missingness matrix\n",
        "msno.matrix(banking)\n",
        "plt.show()\n",
        "\n",
        "# Isolate missing and non missing values of inv_amount\n",
        "missing_investors = banking[banking['inv_amount'].isna()]\n",
        "investors = banking[~banking['inv_amount'].isna()]\n",
        "\n",
        "# Sort banking by age and visualize\n",
        "banking_sorted = banking.sort_values(by='age')\n",
        "msno.matrix(banking_sorted)\n",
        "plt.show()\n",
        "```\n",
        "2\n",
        "```\n",
        "# Drop missing values of cust_id\n",
        "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
        "\n",
        "# Compute estimated acct_amount\n",
        "acct_imp = banking_fullid['inv_amount'] * 5\n",
        "\n",
        "# Impute missing acct_amount with corresponding acct_imp\n",
        "banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})\n",
        "\n",
        "# Print number of missing values\n",
        "print(banking_imputed.isna().sum())\n",
        "```"
      ],
      "metadata": {
        "id": "TQJVWK9ilgMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Record linkage\n",
        "Record linkage is a powerful technique used to merge multiple datasets together, used when values have typos or different spellings.\n",
        "\n",
        "### Simple string comparison\n",
        "```\n",
        "from thefuzz import fuzz\n",
        "\n",
        "# compare reading vs reeding\n",
        "fuzz.WRatio('Reeding', 'Reading')\n",
        "```\n",
        "\\\n",
        "What if we have so many inconsistent categories. We can easily do with String Similarity.\n",
        "\\\n",
        "\\\n",
        "Let's say we have df named survey conting answers from respondentsfrom the state NYC and California asking how likely are you to move on a scale of 0 to 5. The state was free text and contains 100 of typos. We'll use string similarity.\n",
        "\\\n",
        "We also have category DF containing the correct categories for each state.\n",
        "\n",
        "```\n",
        "# For each state category\n",
        "for state in categories['state']:\n",
        "\n",
        "  # Find potential matches in states with typoes\n",
        "  matches = process.extract(state, survey['state'], limit=survey.shape[0])\n",
        "\n",
        "  # For each potential match match\n",
        "  for potential_match in matches:\n",
        "\n",
        "    # If high similarity score\n",
        "    if potential_match[1] >= 80:\n",
        "\n",
        "      # Replace typo with correct category\n",
        "      survey.loc[survey['state'] == potential_match[0], 'state'] = state\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Import process from thefuzz\n",
        "from thefuzz import process\n",
        "\n",
        "# Store the unique values of cuisine_type in unique_types\n",
        "unique_types = restaurants['cuisine_type'].unique()\n",
        "\n",
        "# Calculate similarity of 'asian' to all values of unique_types\n",
        "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# # Calculate similarity of 'american' to all values of unique_types\n",
        "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
        "\n",
        "# # Calculate similarity of 'italian' to all values of unique_types\n",
        "print(process.extract('italian', unique_types, limit = len(unique_types)))\n",
        "```\n",
        "Remapping categories II\n",
        "1\n",
        "```\n",
        "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
        "matches = process.extract('italian', restaurants['cuisine_type'], limit=restaurants.shape[0])\n",
        "\n",
        "# Inspect the first 5 matches\n",
        "print(matches[0:5])\n",
        "```\n",
        "2\n",
        "```\n",
        "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
        "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "# Iterate through the list of matches to italian\n",
        "for match in matches:\n",
        "  # Check whether the similarity score is greater than or equal to 80\n",
        "  # if match > 80:\n",
        "  if match[1] >= 80:\n",
        "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "    restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'\n",
        "```\n",
        "3 you have access to a categories list containing the correct cuisine types ('italian', 'asian', and 'american').\n",
        "```\n",
        "# Iterate through categories\n",
        "for cuisine in categories:  \n",
        "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
        "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
        "\n",
        "  # Iterate through the list of matches\n",
        "  for match in matches:\n",
        "     # Check whether the similarity score is greater than or equal to 80\n",
        "    if match[1] >= 80:\n",
        "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
        "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
        "      \n",
        "# Inspect the final result\n",
        "print(restaurants['cuisine_type'].unique())\n",
        "```\n",
        "\n",
        "## Rocord Linkage\n",
        "Record linkage is the act of linking data from different sources regarding the same entity.\\\n",
        "Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them. using *recordlinkage* package.\n",
        "\n",
        "```\n",
        "# import recordlinkage\n",
        "import recordlinkage\n",
        "\n",
        "# Creating indexing object\n",
        "indexer = recordlinkage.Index()\n",
        "\n",
        "# Generate pairs blocked on state col\n",
        "indexer.block('state')\n",
        "pairs = indexer.index(census_A, census_B)\n",
        "\n",
        "## output is a pandas multi index object containing paris of row indices from both dfs\n",
        "```\n",
        "find potential matches\n",
        "```\n",
        "# Generate a pairs\n",
        "paris = indexer.index(census_A, census_B)\n",
        "\n",
        "# Create a compare object\n",
        "compare_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches for pairs of date_of_birth and state\n",
        "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
        "compare_cl.exact('state', 'state', label='state')\n",
        "\n",
        "# Find similar matches for pairs of surname and address_1 using string similarity\n",
        "compare_cl.string('surname', 'surname', threshold=0.85, label='surname')\n",
        "compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n",
        "\n",
        "# Find matches\n",
        "potential_matches = compare_cl.compute(pairs, cencus_A, census_B)\n",
        "\n",
        "## output is multi index df. cols are compared, value 1 = match, 0 = not a match\n",
        "```\n",
        "Find potential matches. We need to filter.\n",
        "```\n",
        "potential_matches[potential_matches.sum(axis=1) => 2]\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Create an indexer and object and find possible pairs\n",
        "indexer = recordlinkage.Index()\n",
        "indexer\n",
        "# Block pairing on cuisine_type\n",
        "indexer.block('cuisine_type')\n",
        "\n",
        "# Generate pairs\n",
        "pairs = indexer.index(restaurants, restaurants_new)\n",
        "\n",
        "# Create a comparison object\n",
        "comp_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches on city, cuisine_types -\n",
        "comp_cl.exact('city', 'city', label='city')\n",
        "comp_cl.exact('cuisine_type', 'cuisine_type', label='cuisine_type')\n",
        "\n",
        "# Find similar matches of rest_name\n",
        "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8)\n",
        "\n",
        "# Get potential matches and print\n",
        "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
        "print(potential_matches)\n",
        "\n",
        "# 3 because I need to have matches in all my cols.\n",
        "potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
        "```\n",
        "\n",
        "## Linking DataFrames\n",
        "We've already generated apirs, compared 4 of cols, 2 for exact matches and 2 for str similarity. finally, found potential matches. Now it's time to link both census dfs.\n",
        "\\\n",
        "Get the indices\n",
        "```\n",
        "matches.index\n",
        "\n",
        "# Get indices from census_B only\n",
        "duplicate_rows = matches.index.get_level_values(1)\n",
        "print(census_B_index)\n",
        "```\n",
        "\n",
        "```\n",
        "# finding duplicate in census_B\n",
        "census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n",
        "\n",
        "# finding new rows in census_B (non duplciates)\n",
        "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
        "\n",
        "# Link the DFs\n",
        "full_census = census_A.append(census_B_new)\n",
        "```\n",
        "\n",
        "full code of bringing full census result\n",
        "```\n",
        "# import recordlinkage\n",
        "import recordlinkage\n",
        "\n",
        "# Creating indexing object\n",
        "indexer = recordlinkage.Index()\n",
        "\n",
        "# Generate pairs blocked on state col\n",
        "indexer.block('state')\n",
        "\n",
        "full_paris = indexer.index(census_A, census_B)\n",
        "\n",
        "# Create a compare object\n",
        "compare_cl = recordlinkage.Compare()\n",
        "\n",
        "# Find exact matches for pairs of date_of_birth and state\n",
        "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
        "compare_cl.exact('state', 'state', label='state')\n",
        "\n",
        "# Find similar matches for pairs of surname and address_1 using string similarity\n",
        "compare_cl.string('surname', 'surname', threshold=0.85, label='surname')\n",
        "compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n",
        "\n",
        "# Generate potential matches\n",
        "potential_matches = compare_cl.compute(full_pairs, cencus_A, census_B)\n",
        "\n",
        "# Isolate matches with matching values for 3 or more cols\n",
        "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
        "\n",
        "# Get index for matching census_B rows only\n",
        "duplicate_rows = matches.index.get_level_values(1)\n",
        "\n",
        "# finding new rows in census_B (non duplciates)\n",
        "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
        "\n",
        "# Link the DFs\n",
        "full_census = census_A.append(census_B_new)\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Isolate potential matches with row sum >=3\n",
        "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
        "\n",
        "# Get values of second column index of matches\n",
        "matching_indices = matches.index.get_level_values(1)\n",
        "\n",
        "# Subset restaurants_new based on non-duplicate values\n",
        "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
        "\n",
        "# Append non_dup to restaurants\n",
        "full_restaurants = restaurants.append(non_dup)\n",
        "print(full_restaurants)\n",
        "```"
      ],
      "metadata": {
        "id": "9K5dj2lxwKKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install thefuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObEnFp6_xICX",
        "outputId": "8ee7a5b1-232f-4bf8-a062-1b1c9b20f2a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thefuzz\n",
            "  Downloading thefuzz-0.20.0-py3-none-any.whl (15 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, thefuzz\n",
            "Successfully installed rapidfuzz-3.5.2 thefuzz-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from thefuzz import fuzz\n",
        "\n",
        "# compare reading vs reeding\n",
        "fuzz.WRatio('Reeding', 'Reading')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfsMPhidxBhD",
        "outputId": "12973a9a-06d4-4c28-8934-1984a2505052"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fuzz.WRatio('Houston Rockets', 'Rockets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD4aqF_nxXHc",
        "outputId": "36612bae-ece1-497c-d909-f25e6360e5e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fuzz.WRatio('Houston Rockets vs Los Angeles Lakers', 'Lakers vs Rockets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "885F003qxbin",
        "outputId": "230799a9-b87b-4f10-f712-cb015be1ba37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}