{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMc/xjzhZ/nRyohOtpAB8mf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imppel-9704/de_track_datacamp/blob/main/l14_Streamlined_Data_Ingestion_with_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlined Data Ingestion with pandas"
      ],
      "metadata": {
        "id": "vcvMbIDE7vZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to flat files\n",
        "\n",
        "- Simple, Easy to product format\n",
        "- Data stored as plain text (no formatting)\n",
        "- One row per line\n",
        "- Values for different fields are separated by a delimiter\n",
        "- Most common flat file type: comma-separated values\n",
        "- One pandas funtion to load them all: read_csv()\n",
        "\n",
        "### Loading CSVs\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "tax_data = pd.read_csv(\"us_tax_data_2016.csv\")\n",
        "tax_data.head()\n",
        "```\n",
        "\n",
        "### Loading other flat files\n",
        "- Specify a different delimiter with sep\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "tax_data = pd.read_csv(\"us_tax_data_2016.tsv\", sep=\"\\t\")\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Import pandas with the alias pd\n",
        "import pandas as pd\n",
        "\n",
        "# Load TSV using the sep keyword argument to set delimiter\n",
        "data = pd.read_csv(\"vt_tax_data_2016.tsv\", sep=\"\\t\")\n",
        "\n",
        "# Plot the total number of tax returns by income group\n",
        "counts = data.groupby(\"agi_stub\").N1.sum()\n",
        "counts.plot.bar()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Modifying flat file imports\n",
        "Limit the amount of data imported and naming columns\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "tax_data = pd.read_csv(\"us_tax_data_2016.tsv\", sep=\"\\t\")\n",
        "\n",
        "print(tax_data.shape())\n",
        "## output shows amount of columns and rows.\n",
        "```\n",
        "\n",
        "Limiting Column\n",
        "- Choose column to load with the ```usecols``` keyword argument\n",
        "- Accepts a list of column numbers or names, or a function to filter column names\n",
        "\n",
        "```\n",
        "col_names = ['STATEFIPS', 'STATE', 'zipcode', 'agi_stub', 'N1']\n",
        "col_nums = [0, 1, 2, 3, 4]\n",
        "\n",
        "# choose cols to load by name\n",
        "tax_data_1 = pd.read_csv(\"us_tax_data_2016.tsv\", usecols=col_names)\n",
        "\n",
        "# choose cols to load by number\n",
        "tax_data_2 = pd.read_csv(\"us_tax_data_2016.tsv\", usecols=col_nums)\n",
        "\n",
        "print(tax_data_1.equls(tax_data_2))\n",
        "```\n",
        "\n",
        "Limiting Column\n",
        "- Choose rows to load with the ```nrows``` keyword argument\n",
        "\n",
        "```\n",
        "tax_data_first1000 = pd.read_csv(\"us_tax_data_2016.tsv\", nrows=1000)\n",
        "```\n",
        "\n",
        "- ```skiprows``` accepts a list of row numbers, a number of rows, or function to filter rows.\n",
        "- Set ```header=None``` so pandas knows there are no column names.\n",
        "\n",
        "```\n",
        "tax_data_next500 = pd.read_csv(\"us_tax_data_2016.tsv\",\n",
        "                              nrows=500,\n",
        "                              skiprows=1000,\n",
        "                              header=None)\n",
        "```\n",
        "\n",
        "Assigning column names\n",
        "- Supply column names by passing a list to the ```names``` argument\n",
        "- The list MUST have a name for every column in your data\n",
        "- If you only need to rename a few columns, do it after the import!\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Create dataframe of next 500 rows with labeled columns\n",
        "vt_data_next500 = pd.read_csv(\"vt_tax_data_2016.csv\",\n",
        "                       \t\t  nrows=500,\n",
        "                       \t\t  skiprows=500,\n",
        "                       \t\t  header=None,\n",
        "                       \t\t  names=list(vt_data_first500))\n",
        "\n",
        "# View the Vermont dataframes to confirm they're different\n",
        "print(vt_data_first500.head())\n",
        "print(vt_data_next500.head())\n",
        "```\n",
        "\n",
        "## Handling errors and missing data\n",
        "Common flat file import issues\n",
        "- Column data types are wrong\n",
        "- Values are missing\n",
        "- Records that connot be read by pandas\n",
        "\n",
        "Specifying Data Types\n",
        "- Pandas automatically infers column data types\n",
        "\n",
        "```\n",
        "print(tax_data.dtypes)\n",
        "```\n",
        "\n",
        "- User the dtype keyword argument to specify column data types\n",
        "- dtype takes a dictionary of column names and data types\n",
        "\n",
        "```\n",
        "tax_data = pd.read_csv(\"us_tax_data_2016.csv\", dtype={\"zipcode\": str})\n",
        "print(tax_data.dtypes)\n",
        "\n",
        "## zipcode will be str\n",
        "```\n",
        "\n",
        "Customizing Missing data values\n",
        "- pandas automatically interprets somes values as missing or NA\n",
        "- Use the ```na_values``` keyword argument to set custom missing values\n",
        "- Can pass a single value, list, or dictionary of columns and values\n",
        "\n",
        "```\n",
        "tax_data = pd.read_csv(\"us_tax_data_2016.csv\", na_values={\"zipcode\": 0})\n",
        "print(tax_data[tax_data.zipcode.isna()])\n",
        "\n",
        "## it will be 0 instead NA\n",
        "```\n",
        "\n",
        "Lines with errors\n",
        "- Set ```error_bad_lines=False``` to skip unparseable records\n",
        "- Set ```warn_bad_lines=True``` to see messages when records are skipped\n",
        "\n",
        "```\n",
        "tax_data = pd.read_csv(\"us_tax_data_2016_corrupt.csv\",\n",
        "                      error_bad_lines=False,\n",
        "                      warn_bad_lines=True)\n",
        "```\n",
        "\n",
        "\n",
        "### Exercise\n",
        "1\n",
        "```\n",
        "# Create dict specifying data types for agi_stub and zipcode\n",
        "data_types = {'agi_stub': \"category\",\n",
        "\t\t\t  'zipcode': str}\n",
        "\n",
        "# Load csv using dtype to set correct data types\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\", dtype=data_types)\n",
        "\n",
        "# Print data types of resulting frame\n",
        "print(data.dtypes.head())\n",
        "```\n",
        "2\n",
        "```\n",
        "# Create dict specifying that 0s in zipcode are NA values\n",
        "null_values = {'zipcode': 0}\n",
        "\n",
        "# Load csv using na_values keyword argument\n",
        "data = pd.read_csv(\"vt_tax_data_2016.csv\",\n",
        "                   na_values=null_values)\n",
        "\n",
        "# View rows with NA ZIP codes\n",
        "print(data[data.zipcode.isna()])\n",
        "```"
      ],
      "metadata": {
        "id": "4RfoPl4zlocu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to spreadsheets\n",
        "### Spreadsheets\n",
        "- Also known as Excel files\n",
        "- Data stored in tabular form, with cells arranged in rows and columns\n",
        "- Unlike flat files, can have formatting and formulas\n",
        "- Multiple spreadsheets can exist in a workbook\n",
        "\n",
        "Loading Spreadsheets\n",
        "- Spreadsheets have their own loading function in pandas: ```read_excel()```\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "# Read the excel file\n",
        "survey_data = pd.read_excel(\"fcc_survey.xlsx\")\n",
        "\n",
        "print(suvey_data.head())\n",
        "```\n",
        "\n",
        "Loading select columns and rows\n",
        "- read_excel() have many keyword arguments in common with read_csv()\n",
        "  - nrows: limit number of rows to load\n",
        "  - skiprows: specify number of rows or row numbers to skip\n",
        "  - usecols: choose columns by name, positional number, or letter\n",
        "\n",
        "```\n",
        "# Read columns W-AB and AR of file, skipping metadata header\n",
        "survey_data = pd.read_excel(\"fcc_survey_with_header.xlsx\",\n",
        "                            skiprows=2,\n",
        "                            usecols=\"W:AB, AR\")\n",
        "\n",
        "# View data\n",
        "print(survey_data.head())\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "```\n",
        "# Create string of lettered columns to load\n",
        "col_string = \"AD, AW:BA\"\n",
        "\n",
        "# Load data with skiprows and usecols set\n",
        "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\",\n",
        "                        skiprows=2,\n",
        "                        usecols=col_string)\n",
        "\n",
        "# View the names of the columns selected\n",
        "print(survey_responses.columns)\n",
        "```\n",
        "\n",
        "## Getting data from multiple worksheets\n",
        "\n",
        "Selecting sheets to load\n",
        "- read_excel() loads the first sheet in an Excel file by default\n",
        "- Use the sheet_name keyword argument to load other sheets\n",
        "- Specify spreadsheets by name and/or (zero-indexed) position number\n",
        "- Pass a list of names/numbers to load more than one sheet at a time\n",
        "- Any arguments passed to read_excel() apply to all sheets read\n",
        "\n",
        "Loading select sheets\n",
        "```\n",
        "survey_data_sheet2 = pd.read_excel(\"fcc_survey_headers.xlsx\", sheet_name=1)\n",
        "\n",
        "survey_data_sheet2017 = pd.read_excel(\"fcc_survey_headers.xlsx\", sheet_name=\"2017\")\n",
        "\n",
        "print(survey_data_sheet2.equals(survey_data_sheet2017))\n",
        "```\n",
        "\n",
        "Loading all sheet\n",
        "- Passing sheet_name=None to read_excel() reads all sheets in a workbook\n",
        "\n",
        "```\n",
        "survey_responses = pd.read_excel(\"fcc_survey_headers.xlsx\", sheet_name=None)\n",
        "\n",
        "print(type(survey_responses))\n",
        "## <class 'colletions.OrderedDict'>\n",
        "```\n",
        "\n",
        "```\n",
        "for k, v in survey_responses.items(): print(k, type(v))\n",
        "\n",
        "2016 <class 'pandas.core.frame.DataFrame'>\n",
        "2017 <class 'pandas.core.frame.DataFrame'>\n",
        "```\n",
        "\n",
        "Putting it all together\n",
        "```\n",
        "# Create empty dataframe to hold all loaded sheets\n",
        "all_responses = pd.DataFrame()\n",
        "\n",
        "# Iterate through dataframes in dictionary\n",
        "for sheet_name, frame in survey_responses.items:\n",
        "\n",
        "  # Add a column so we know which year data is from\n",
        "  frame[\"Year\"] = sheet_name\n",
        "\n",
        "  # Add each dataframe to all_resonses all_resonses = all_responses.append(frame)\n",
        "  \n",
        "# View years in data\n",
        "print(all_responses.Year.unique())\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "1\n",
        "```\n",
        "# Create df from second worksheet by referencing its position\n",
        "responses_2017 = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                               sheet_name=1)\n",
        "\n",
        "# Graph where people would like to get a developer job\n",
        "job_prefs = responses_2017.groupby(\"JobPref\").JobPref.count()\n",
        "job_prefs.plot.barh()\n",
        "plt.show()\n",
        "```\n",
        "2\n",
        "```\n",
        "# Load both the 2016 and 2017 sheets by name\n",
        "all_survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                                sheet_name=['2016', '2017'])\n",
        "\n",
        "# View the data type of all_survey_data\n",
        "print(type(all_survey_data))\n",
        "```\n",
        "3\n",
        "```\n",
        "# Create an empty dataframe\n",
        "all_responses = pd.DataFrame()\n",
        "\n",
        "# Set up for loop to iterate through values in responses\n",
        "for df in responses.values():\n",
        "  # Print the number of rows being added\n",
        "  print(\"Adding {} rows\".format(df.shape[0]))\n",
        "  # Append df to all_responses, assign result\n",
        "  all_responses = all_responses.append(df)\n",
        "\n",
        "# Graph employment statuses in sample\n",
        "counts = all_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\n",
        "counts.plot.barh()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Modifying imports: true/false data\n",
        "Boolean data\n",
        "- True / False data\n",
        "\n",
        "Pandas and Booleans\n",
        "- Pandas loads True / False columns as float data by default\n",
        "- Specify a column should be bool with read_excel()'s dtype argument\n",
        "- Boolean columns can only have True and False valeus\n",
        "- NA / missing values in Boolean columnsa re changed to True\n",
        "- pandas automatically recognizes some  values as True / False in Boolean columns\n",
        "- Unrecognized values in a Boolean column are also changed to True\n",
        "\n",
        "Setting custom True / False values\n",
        "- Use read_excel()'s true_values argument to set custom True values\n",
        "- Use false_values to set custom False values\n",
        "- Each takes a list3 of values to treat as True / False, respectively\n",
        "- Custom True / False values are only applied to columns set as Boolean\n",
        "\n",
        "Boolean Considerations\n",
        "- Are there missing values, or could there be in the future?\n",
        "- How will this column be used in analysis?\n",
        "- What would happen if a value were incorrectly coded as True?\n",
        "- Could the data be modeled another way (e.g., as floats or integers)?\n",
        "\n",
        "### Exercise\n",
        "1 Set Boolean columns\n",
        "```\n",
        "# Load the data\n",
        "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\")\n",
        "\n",
        "# Count NA values in each column\n",
        "print(survey_data.isna().sum())\n",
        "```\n",
        "```\n",
        "# Set dtype to load appropriate column(s) as Boolean data\n",
        "survey_data = pd.read_excel(\"fcc_survey_subset.xlsx\",\n",
        "                            dtype={\"HasDebt\":bool})\n",
        "\n",
        "# View financial burdens by Boolean group\n",
        "print(survey_data.groupby('HasDebt').sum())\n",
        "```\n",
        "2 Set custom true/false values\n",
        "```\n",
        "# Load file with Yes as a True value and No as a False value\n",
        "survey_subset = pd.read_excel(\"fcc_survey_yn_data.xlsx\",\n",
        "                              dtype={\"HasDebt\": bool,\n",
        "                              \"AttendedBootCampYesNo\": bool},\n",
        "                              true_values=['Yes'],\n",
        "                              false_values=['No'])\n",
        "\n",
        "# View the data\n",
        "print(survey_subset.head())\n",
        "```\n",
        "\n",
        "## Modifying imports: parsing dates\n",
        "Date and Time Data\n",
        "- Dates and times have their own data type and internal representation\n",
        "- Datetime values can be translated into string representations\n",
        "- Common set of codes to describe datetime string formatting\n",
        "\n",
        "pandas and Datetimes\n",
        "- Datetime columns are loaded as objects (string) by default\n",
        "- Sepecify that columns have datetimes with the parse_dates argument (not dtype!)\n",
        "- parse_dates can accept:\n",
        "  - a list of column names or numbers to parse\n",
        "  - a list containg lists of columns to combine and parse\n",
        "  - a dictionary where keys are new column names and values are lists of columns to parse together\n",
        "\n",
        "Parsing Dates in standard format\n",
        "```\n",
        "# List columns of dates to parse\n",
        "date_cols = ['Part1StartTime', 'Part1EndTime']\n",
        "\n",
        "# Load file, parsing standard datetime columns\n",
        "suvey_df = pd.read_excel('fcc_survey.xlsx', parse_dates=date_cols)\n",
        "```\n",
        "\n",
        "Non-Standard Dates\n",
        "- parse_dates doesn't work with non-standard datetime formats\n",
        "- Use pd.to_datetime() after loading data if parse_dates won't work\n",
        "- to_datetime() arguments:\n",
        "  - DataFrame and column to convert\n",
        "  - format: string representation of datetime format\n",
        "\n",
        "Datetime formatting\n",
        "- Describe datetime string formatting with codes and characters\n",
        "- Refer to strftime.org for the full list\n",
        "Code: %Y Meaning: Year (4-digit) Example: 2023\n",
        "Code: %m Meaning: Month (zero-padded) Example: 09\n",
        "Code: %d Meaning: Day (zero-padded) Example: 05\n",
        "Code: %H Meaning: Hour (24-hour clock) Example: 21\n",
        "Code: %M Meaning: Minute (zero-padded) Example: 09\n",
        "Code: %S Meaning: Second (zero-padded) Example: 05\n",
        "\n",
        "Parsing Non-standard dates\n",
        "```\n",
        "# values in col was 03292016 21:27:25\n",
        "format_string = \"%m%d%Y %H:%M:%S\"\n",
        "\n",
        "survey_df[\"Part2EndTime\"] = pd.to_datetime(survey_df[\"Part2EndTime\"], format=format_string)\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "1 Parse simple dates\n",
        "```\n",
        "# Load file, with Part1StartTime parsed as datetime data\n",
        "survey_data = pd.read_excel(\"fcc_survey.xlsx\",\n",
        "                            parse_dates=['Part1StartTime'])\n",
        "\n",
        "# Print first few values of Part1StartTime\n",
        "print(survey_data.Part1StartTime.head())\n",
        "```\n",
        "\n",
        "2 Get datetimes from multiple columns\n",
        "```\n",
        "# Create dict of columns to combine into new datetime column\n",
        "datetime_cols = {\"Part2Start\": [\"Part2StartDate\", \"Part2StartTime\"]}\n",
        "\n",
        "\n",
        "# Load file, supplying the dict to parse_dates\n",
        "survey_data = pd.read_excel(\"fcc_survey_dts.xlsx\",\n",
        "                            parse_dates=datetime_cols)\n",
        "\n",
        "# View summary statistics about Part2Start\n",
        "print(survey_data.Part2Start.describe())\n",
        "```\n",
        "\n",
        "3 Parse non-standard date formats\n",
        "```\n",
        "# Parse datetimes and assign result back to Part2EndTime\n",
        "survey_data[\"Part2EndTime\"] = pd.to_datetime(survey_data[\"Part2EndTime\"],\n",
        "                                             format=\"%m%d%Y %H:%M:%S\")\n",
        "\n",
        "# Print first few values of Part2EndTime\n",
        "print(survey_data[\"Part2EndTime\"].head())\n",
        "```"
      ],
      "metadata": {
        "id": "Df-1q0S-10pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to databases\n",
        "\n",
        "### Relational Database\n",
        "- Data about entities is organized into tables\n",
        "- Each row or record is an instance of an entity\n",
        "- Each column has information about an attribute\n",
        "- Tables can be linked to each other via unique keys\n",
        "- Support more data, multiple simultaneous users, and data quality controls\n",
        "- Data types are specified for each column\n",
        "- SQL (Structured Query Language) to interact with databases\n",
        "\n",
        "Connecting to Databases\n",
        "- 2 step process:\n",
        "  1. Create way to connect to database\n",
        "  2. Query database\n",
        "\n",
        "Creating a Database Engine\n",
        "```sqlalchemy``` 's ```create_engine()``` makes an engine to handle database connections\n",
        "- Needs string URL of database to connect to - SQLite URL format: sqlite:///filename.db\n",
        "\n",
        "Querying Databases\n",
        "- pd.read_sql(query, engine) to load in data from a database\n",
        "- Arguments\n",
        "  - query: String containing SQL query to run or table to load\n",
        "  - engine: Connection / database engine object\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create database engine to manage connections\n",
        "engine = create_engine(\"sqlite:///data.db\")\n",
        "# Load entire weather table by table name\n",
        "weather = pd.read_sql(\"weather\", engine)\n",
        "```\n",
        "OR\n",
        "```\n",
        "# Create database engine to manage connections\n",
        "engine = create_engine(\"sqlite:///data.db\")\n",
        "# Load entire weather table by table name\n",
        "weather = pd.read_sql(\"SELECT * FROM weather\", engine)\n",
        "\n",
        "print(weather.head())\n",
        "```\n",
        "\n",
        "### Refining imports with SQL queries\n",
        "WHERE Clauses\n",
        "- Use a WHERE clause to selectively import records\n",
        "\n",
        "Filtering by Numbers\n",
        "- Compare numbers with mathematical opeators\n",
        "  - =\n",
        "  - '>'and >=\n",
        "  - < and <=\n",
        "  - <> (not equal to)\n",
        "\n",
        "Filtering text\n",
        "- Match exact strings with the = sign and the text to match\n",
        "- String matching is case-sensitive\n",
        "\n",
        "```\n",
        "# Create query for records with max temps <= 32 or snow >= 1\n",
        "query = \"\"\"\n",
        "SELECT *\n",
        "  FROM weather\n",
        "  WHERE tmax <= 32\n",
        "  OR snow >= 1;\n",
        "\"\"\"\n",
        "\n",
        "# Query database and assign result to wintry_days\n",
        "wintry_days = pd.read_sql(query, engine)\n",
        "\n",
        "# View summary stats about the temperatures\n",
        "print(wintry_days.describe())\n",
        "```\n",
        "\n",
        "### More complex SQL queries\n",
        "Getting DISTINCT values\n",
        "- Get unique values for one or more columns with SELECT DISTINCT\n",
        "- Syntax: ```SELECT DISTINCT col FROM table;```\n",
        "- Remove duplicate records: ```SELECT DISTINCT * FROM table;```\n",
        "\n",
        "Aggregate functions\n",
        "- Query a database directly for descriptive statistics\n",
        "\n",
        "GROUP BY\n",
        "- Aggreagate functions calculate a single summary statistic by default\n",
        "- Summarize data by categories with GROUP BY statements\n",
        "- Remember to also select the column you're grouping by!\n",
        "\n",
        "### Loading multiple tables with joins\n",
        "Keys\n",
        "- Database records have unique identifiers, or keys\n",
        "\n",
        "Joining Tables\n",
        "- Use dot notation table.column when working with multiple tables\n",
        "- Default join only returns records whose key values appear in both tables\n",
        "- Make sure join keys are the same data type or nothing will match\n",
        "\n",
        "```\n",
        "query = \"\"\"SELECT hd311calls.borough,\n",
        "                  COUNT(*),\n",
        "                  boro_census.total_population,\n",
        "                  boro_census.housing_units\n",
        "          FROM hpd311calls\n",
        "          JOIN boro_census\n",
        "          ON hpd311calls.borough = boro_census.borough\n",
        "          GROUP BY hpd311calls.borough;\"\"\"\n",
        "call_counts = pd.read_sql(query, engine)\n",
        "print(call_counts)\n",
        "```"
      ],
      "metadata": {
        "id": "rdEVTmp87io6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing JSON Data and Working with APIs\n",
        "### Introduction to JSON\n",
        "\n",
        "Javascript Object Notation (json)\n",
        "- Common web data format\n",
        "- Not tabular\n",
        "  - Records don't have to all have the same set of attributes\n",
        "- Data organized into collections of objects\n",
        "- Objects are collections of attribute-value pairs\n",
        "- Nested JSON: objects within objects\n",
        "\n",
        "Reading JSON data\n",
        "- ```read_json()```\n",
        "  - Takes a string path JSON _or_ JSON data as a string\n",
        "  - Specify data types with dtype keyword argument\n",
        "  - orient keyword argument to flag uncommon JSON data layouts\n",
        "    - possible values in pandas documentation\n",
        "\n",
        "Data Orientation\n",
        "- JSON data isn't tabular\n",
        "  - pandas guesses how to arrange it in a table\n",
        "- pandas can automatically handle common orientations\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "death_causes = pd.read_json(\"nyc_death_causes.json\", orient=\"split\")\n",
        "print(death_causes.head())\n",
        "```\n",
        "\n",
        "### Exercise\n",
        "Normal load json\n",
        "```\n",
        "# Load pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Load the daily report to a dataframe\n",
        "pop_in_shelters = pd.read_json(\"dhs_daily_report.json\")\n",
        "\n",
        "# View summary stats about pop_in_shelters\n",
        "print(pop_in_shelters.describe())\n",
        "```\n",
        "Load with argument\n",
        "```\n",
        "try:\n",
        "    # Load the JSON with orient specified\n",
        "    df = pd.read_json(\"dhs_report_reformatted.json\",\n",
        "                      orient=\"split\")\n",
        "    \n",
        "    # Plot total population in shelters over time\n",
        "    df[\"date_of_census\"] = pd.to_datetime(df[\"date_of_census\"])\n",
        "    df.plot(x=\"date_of_census\",\n",
        "            y=\"total_individuals_in_shelter\")\n",
        "    plt.show()\n",
        "    \n",
        "except ValueError:\n",
        "    print(\"pandas could not parse the JSON.\")\n",
        "```\n",
        "\n",
        "## Introduction to APIs\n",
        "```\n",
        "import requests\n",
        "import pandas as pd\n",
        "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
        "\n",
        "# Set up parameter dictionary according to documentation\n",
        "params = {\"term\": \"bookstore\", \"location\": \"San Francisco\"}\n",
        "\n",
        "# Set up header dictionary w/ API key according to documentation\n",
        "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
        "\n",
        "# Call the API\n",
        "response = requests.get(api_url,\n",
        "                        params=params,\n",
        "                        headers=headers)\n",
        "\n",
        "# Isolate the JSON data from the response object\n",
        "data = response.json()\n",
        "print(data)\n",
        "\n",
        "# Load businesses data to a dataframe\n",
        "bookstores = pd.DataFrame(data[\"businesses\"])\n",
        "print(bookstores.head(2))\n",
        "```\n",
        "\n",
        "## Working with nested JSONs\n",
        "### Nested JSONs\n",
        "- JSONs contain objects with attribute-value pairs\n",
        "- A JSON is nested when the value itself is an object\n",
        "\n",
        "### pandas.io.json\n",
        "- pandas.io.json submodule has tools for reading and writing JSON\n",
        "  - Needs its own import statement\n",
        "- json_normalize()\n",
        "  - Takes a dictionary/list of dictionaries (like pd.DataFrame() does)\n",
        "  - Returns a flattened dataframe - Default flattened column name pattern: attribute.nestedattribute\n",
        "  - Choose a different reparator with the sep argument\n",
        "\n",
        "### Loading Nested JSON data\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import requests\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "# Set up headers, params, and API endpoint\n",
        "\n",
        "api_url = \"https://api.yelp.com/v3/businesses/search\"\n",
        "headers = {\"Authorization\": \"Bearer {}\".format(api_key)}\n",
        "params = {\"term\": \"bookstore\", \"location\": \"San Francisco\"}\n",
        "\n",
        "# Make the API call and extract the JSON data\n",
        "response = requests.get(api_url, headers=headers, params=params)\n",
        "data = response.json()\n",
        "\n",
        "# Flatten data and load to dataframe, with _ separators\n",
        "bookstores = json_normalize(data['businesses'], sep='_')\n",
        "print(list(bookstores))\n",
        "\n",
        "print(bookstores.categories.head())\n",
        "```\n",
        "\n",
        "- json_normalize()\n",
        "  - record_path: string/list of string attributes to nested data\n",
        "  - meta: list of other attributes to load to dataframe\n",
        "  - meta_prefix: string to prefix to meta column names\n",
        "\n",
        "```\n",
        "bookstores = json_normalize(data[\"businesses\"],\n",
        "                            sep=\"_\",\n",
        "                            record_path=\"categories\",\n",
        "                            meta=[\"name\",\n",
        "                                  \"alias\",\n",
        "                                  \"rating\",\n",
        "                                  [\"coordinates\", \"latitude\"],\n",
        "                                  [\"coordinates\", \"longitude\"]],\n",
        "                            meta_prefix=\"biz_\")\n",
        "```\n",
        "\n",
        "## Combining multiple datasets\n",
        "### Appending\n",
        "- Use case: adding rows from one dataframe to another\n",
        "- append() - Dataframe method - Syntax: ```df1.append(df2)```\n",
        "- Set ```ignore_index``` to True to renumber rows\n",
        "\n",
        "### Merging\n",
        "- User case: combining datasets to add related columns - Datasets have key column(s) with common values\n",
        "- ```merge()``` : ```pandas``` version of a SQL join\n",
        "- merge()\n",
        "  - Both a pandas function and a dataframe method\n",
        "- df.merge() arguments - Second dataframe to merge\n",
        "  - Columns to merge on - on if names are the same in both dataframes\n",
        "    - left_on and right_on if key names differ\n",
        "    - Key columns should be the same data type\n",
        "- Default merge() behavior: return only values that are in both datasets\n",
        "- One record for each value match between dataframes\n",
        "  - Multiple matches = multiple records\n",
        "\n",
        "```\n",
        "# Merge crosswalk into cafes on their zip code fields\n",
        "cafes_with_pumas = cafes.merge(crosswalk, left_on=\"location_zip_code\", right_on=\"zipcode\")\n",
        "\n",
        "# Merge pop_data into cafes_with_pumas on puma field\n",
        "cafes_with_pop = cafes_with_pumas.merge(pop_data, on=\"puma\")\n",
        "\n",
        "# View the data\n",
        "print(cafes_with_pop.head())\n",
        "```"
      ],
      "metadata": {
        "id": "tJFW8MXwexyo"
      }
    }
  ]
}